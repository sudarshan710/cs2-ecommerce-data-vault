# Ecommerce Data Vault & Delta Lake Architecture

This project implements a scalable and robust data architecture for ecommerce analytics using Delta Lake and Data Vault 2.0. It transforms raw operational data into clean, historized, and query-ready star schemas‚Äîdesigned for real-time reporting and seamless BI integration.


## üöÄ Features

- **Raw to Curated Pipeline** with schema evolution, surrogate keys, and hash generation
- **Data Vault Modeling** (Hub, Link, Satellite tables)
- **Delta Lake Capabilities**: Time Travel, GDPR delete compliance, and Vacuum retention
- **Zero-Copy Star Schema Views**
- Generate **Schema Views** for **Business Intelligence (BI) Integration** with Tableau, Power BI, etc.

## üìÅ Architecture Overview

```text
Data Sources
  ‚îî‚îÄ Customer Master, Product Catalog, Sales Orders, Returns & Refunds

‚¨áÔ∏è Pipeline automation (hash generation, incremental loads)

‚¨áÔ∏è Raw Delta Lake Zone
  ‚îî‚îÄ Schema evolution, merges, surrogate keys

‚¨áÔ∏è Raw Delta Lake Tables
  ‚îî‚îÄ Hub, Link, Satellite tables with audit columns and hash keys

‚¨áÔ∏è Delta Lake Feature Layer
  ‚îî‚îÄ Time Travel, GDPR Delete, Vacuum retention

‚¨áÔ∏è Business Vault Zone
  ‚îî‚îÄ Zero-copy star schema views

‚¨áÔ∏è Publishing Layer
  ‚îî‚îÄ Star schema views

‚¨áÔ∏è Business Reporting / Analytics Zone
  ‚îî‚îÄ Fact & Dimension tables ready for BI tools (Tableau, Power BI)
```

## üìÅ Project Structure

```text
‚îú‚îÄ‚îÄ __pycache__/                                # Python cache files
‚îú‚îÄ‚îÄ artifacts/                                  # Output or artifact files (checkpoints, logs, etc.)
‚îú‚îÄ‚îÄ data_source_folder/                         # Raw source data files
‚îú‚îÄ‚îÄ delta/                                      # Delta Lake tables storage folder
‚îú‚îÄ‚îÄ metastore_db/                               # Metadata storage for Spark SQL (local Derby metastore)
‚îú‚îÄ‚îÄ app.log                                     # Application log file
‚îú‚îÄ‚îÄ derby.log                                   # Derby metastore log file
‚îú‚îÄ‚îÄ driver.py                                   # Main driver script to run the pipeline
‚îú‚îÄ‚îÄ gdpr.py                                     # Script handling GDPR data deletion and compliance
‚îú‚îÄ‚îÄ hubs_links_sats.py                          # Script for creating Data Vault Hub, Link, and Satellite tables
‚îú‚îÄ‚îÄ log_after_clean_ingestion.log               # Sample Log file after clean data ingestion
‚îú‚îÄ‚îÄ log_after_new_customer_order.log            # Sample Log file for new customer orders processing
‚îú‚îÄ‚îÄ logger.py                                   # Logger configuration and utility
‚îú‚îÄ‚îÄ pit_table.py                                # Script managing Point-In-Time (PIT) tables for Data Vault
‚îú‚îÄ‚îÄ scd.log                                     # Slowly Changing Dimension log file                  
‚îú‚îÄ‚îÄ schema_views.py                             # Script creating star schema views from business vault
‚îú‚îÄ‚îÄ star_schema_scd.py                          # Script managing Slowly Changing Dimensions in star schema
‚îú‚îÄ‚îÄ testing.py                                  # Testing scripts and validation
‚îú‚îÄ‚îÄ time_travel.py                              # Script utilizing Delta Lake Time Travel features
```

## ‚öôÔ∏è Technology Stack
- PySpark: 4.0.0
- Delta Lake: 4.0.0
- Hadoop: 3.0.0
- Java: 17

## ‚ñ∂Ô∏è Running the Pipeline

Make sure you have the Delta Lake jars and Spark configured properly. The driver.py script accepts the path to your data source folder as a required positional argument, 
and an optional --step argument to specify which pipeline steps to execute.

### Arguments
- `data_source` (str): Path to your raw data source folder
- `--step` (str list): Steps to run. Options are:
-- `ingest` ‚Äî Data ingestion
-- `vault` ‚Äî Data Vault modeling (hub, link, satellite)
-- `star` ‚Äî Star schema view creation
  
> By default, all steps `["ingest", "vault", "star"]` run.

```
spark-submit \
  --driver-class-path "<path-to-delta-spark-jar>;<path-to-delta-storage-jar>" \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  driver.py <path-to-data-source-folder> --step <step1> <step2> ...
```

Example: Run only the Star Schema step on Windows
```
spark-submit --driver-class-path "C:\spark\jars\delta-spark_2.13-4.0.0.jar;C:\spark\jars\delta-storage-4.0.0.jar" \
  --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" \
  --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog" \
  driver.py "C:\Program Files\sudarshan\cs_ecommerce_data_vault" --step star
```

## üõ†Ô∏è Script Overview


| Script               | Purpose                                                                  |
| -------------------- | ------------------------------------------------------------------------ |
| `driver.py`          | Main entry point for orchestrating all steps (`ingest`, `vault`, `star`) |
| `hubs_links_sats.py` | Creates Hub, Link, and Satellite tables based on Data Vault modeling     |
| `gdpr.py`            | Handles GDPR-compliant deletion using Delta Lake APIs                    |
| `pit_table.py`       | Builds Point-In-Time (PIT) tables for efficient querying                 |
| `schema_views.py`    | Creates business-facing star schema views from the vault                 |
| `star_schema_scd.py` | Manages Slowly Changing Dimensions in star schemas                       |
| `time_travel.py`     | Demonstrates Delta Lake's Time Travel feature                            |
| `logger.py`          | Central logging configuration used across all modules                    |
| `testing.py`         | Custom scripts for testing and validation of entities


## Author

Sudarshan Zunja 

[github.com/sudarshan710](https://github.com/sudarshan710)  

## üì¨ Contact
For questions, collaboration, or support, contact:

Sudarshan Zunja

üìß [sudarshanhope710@gmail.com](sudarshanhope710@gmail.com)

üîó [github.com/sudarshan710](https://github.com/sudarshan710)  
